# Run ComfyUI with ROCm on AMD GPU

*link:README.zh.adoc[中文说明]*

image:https://github.com/YanWenKun/ComfyUI-Docker/actions/workflows/build-rocm.yml/badge.svg["GitHub Workflow Status",link="https://github.com/YanWenKun/ComfyUI-Docker/actions/workflows/build-rocm.yml"]

https://hub.docker.com/r/yanwk/comfyui-boot/tags?name=rocm[View on <Docker Hub>]

## Note - not all-in-one image

Due to GitHub CI error "No space left on device",
I have to shorten the list for installing Python dependencies and the list for downloading Custom Nodes.

If you build this image locally, 
you can uncomment the commented lines in `Dockerfile` and `download.sh`, 
to build an all-in-one image.

## Prepare

* Make sure
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html[Radeon software for Linux with ROCm]
is installed on your Linux host.

## Run

.Run with Docker
[source,sh]
----
mkdir -p storage

docker run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -e CLI_ARGS="" \
  yanwk/comfyui-boot:rocm
----

.Run with Podman
[source,sh]
----
mkdir -p storage

podman run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  -v "$(pwd)"/storage:/root \
  -e CLI_ARGS="" \
  docker.io/yanwk/comfyui-boot:rocm
----

Once the app is loaded, visit http://localhost:8188/

[[hint]]
## If you want to dive in...

(Just notes. Nothing to do with this Docker image)

ROCm has a PyTorch image:

https://hub.docker.com/r/rocm/pytorch

[source,sh]
----
docker pull rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0

mkdir -p storage

docker run -it --rm \
  --name comfyui-rocm \
  --device=/dev/kfd --device=/dev/dri \
  --group-add=video --ipc=host --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --security-opt label=disable \
  -p 8188:8188 \
  --user root \
  --workdir /root/workdir \
  -v "$(pwd)"/storage:/root/workdir \
  rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0 \
  /bin/bash

git clone https://github.com/comfyanonymous/ComfyUI.git

# Or use conda
pip install -r ComfyUI/requirements.txt

# Or python3
python ComfyUI/main.py --listen --port 8188
----

It's big, but if you find it hard to run the container, it may be helpful. As it takes care of PyTorch, the most important part, and you just need to install few more Python packages in order to run ComfyUI.

## Additional notes for Windows users

(Just notes. Nothing to do with this Docker image)

WSL2 supports ROCm and DirectML.

* ROCm

If your GPU is in the
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/compatibility/wsl/wsl_compatibility.html[Compatibility List],
you can either install
https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/wsl/install-radeon.html[Radeon software]
in your WSL2 distro,
or use
<<hint, ROCm PyTorch image>>.

* DirectML

DirectML works for most GPUs (including AMD APU, Intel GPU).
It's slower than ROCm but still faster than CPU.
See: 
link:../docs/wsl-directml.adoc[Run ComfyUI on WSL2 with DirectML]. 

* ZLUDA

This is not using WSL2, it's running natively on Windows. ZLUDA can "translate" CUDA codes to run on AMD GPUs. But as the first step, I recommend to try running SD-WebUI with ZLUDA, it's easier to start with.
